{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGAI_Explainer_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fvM9MeznY0_U",
        "Kf4nX63WVdeX"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SimonTommerup/02805-sgai/blob/main/notebooks/simon_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V06tNTDLiQ5Z"
      },
      "source": [
        "Imports used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRg0HIrTUQ6I"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puF6ABWVUmRi"
      },
      "source": [
        "# 1 Motivation\n",
        "- What is your dataset?\n",
        "- Why did you choose this/these particular dataset(s)?\n",
        "- What was your goal for the end user's experience?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiZnKQFCb-t2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-HscjB8ZdiQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NjyTiF0U3yz"
      },
      "source": [
        "# 2 Basic stats. Let's understand the dataset better\n",
        "- Write about your choices in data cleaning and preprocessing\n",
        "- Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A) **But leave network stats (#nodes, #edges degree...) to next section!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08GSh1ZTdG0C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq7UjHJdZdBW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz3xzEYPUpK-"
      },
      "source": [
        "# 3 Tools, theory and analysis. Describe the process of theory to insight\n",
        "In this section we analyze the above presented data using network science tools and data analysis strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsWy83p_VnYu"
      },
      "source": [
        "## 3.1 Introducing the analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhWRLgwsWBO2"
      },
      "source": [
        "\n",
        "As mentioned and discussed in previous sections, we will be investigating typical users on the subreddit pages of the two 2020 presidential candidates of USA [Trump](https://www.reddit.com/r/donaldtrump/) and [Biden](https://www.reddit.com/https://www.reddit.com/r/joebiden/).\n",
        "\n",
        "Initially we will **Introduction..., then 3.2 then 3.3. then 3.4. (overview!)**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2loFQy-W5zC"
      },
      "source": [
        "### 3.1.2 The bipartite network **Consider: building actual bipartite network?**\n",
        "We will represent our data with a bipartite network of users and subreddits. For our data we have \n",
        "- $U$, the set of users\n",
        "- $S^u$, the set of subreddits which $u \\in U$ has commented on, excluding \n",
        "- $S$, the set of all subreddits commented on by all users.\n",
        "\n",
        "Note that we do not include [Trump](https://www.reddit.com/r/donaldtrump/) and [Biden](https://www.reddit.com/https://www.reddit.com/r/joebiden/) subreddits in $S^u$ and $S$\n",
        "\n",
        "We can assemble this data into a undirected bipartite network $G_{bi}$ with two distjoint sets of nodes $U$ and $S$. A user $u \\in U$ is connected to subreddit $s \\in S$ if $s \\in S^u$. This means that we have a network $G_{bi}$ where a user has a connection to all subreddits which it have commented on. And these subreddits are further connected to every other user, which has also commented on this subreddit. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH9puB0KXVfT"
      },
      "source": [
        "### 3.1.3 Building a weighted network of users\n",
        "In order to apply a wide range of network science tools and data science strategies from 02805, a projection of $G_{bi}$ is considered, constituting a network of users. Specifically, a [simple projection](https://en.wikipedia.org/wiki/Bipartite_network_projection) is considered where users $u_i$ and $u_j$ are connected with weight equal to the size of $(S^{u_i}\\cap S^{u_j})$ - i.e. weight is equal to number of subreddits which both users has commented on. \n",
        "\n",
        "We will be using our constructed pandas dataframe to generate the weighted network $G$ implicit from $G_{bi}$, as it is more simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lM03-j23k2"
      },
      "source": [
        "\n",
        "def get_ids_of_users_with_common_subreddits(user_id, users, used_subreddits):\n",
        "    users_with_common_subreddits = []\n",
        "    for other_user_id in range(len(users)):\n",
        "        if other_user_id != user_id:  # Skip connection to self\n",
        "            for subr in used_subreddits[user_id]:\n",
        "                if subr in used_subreddits[other_user_id]:\n",
        "                    users_with_common_subreddits.append(other_user_id)\n",
        "                    break\n",
        "    return users_with_common_subreddits\n",
        "\n",
        "\n",
        "def get_common_subreddits(user_id, other_user_id, used_subreddits):\n",
        "    common_subreddits = [subreddit for subreddit in used_subreddits[user_id] if subreddit in used_subreddits[other_user_id]]\n",
        "    return common_subreddits\n",
        "\n",
        "  \n",
        "def create_graph(users, used_subreddits, from_subreddits, n_required_subreddits=1):\n",
        "    G = nx.Graph()\n",
        "    # Loop through all users\n",
        "    for user_id in tqdm(range(len(users))):\n",
        "        # Add a node for EVERY user in data set \n",
        "        G.add_node(users[user_id], from_subreddit=from_subreddits[user_id])\n",
        "\n",
        "        # Get all other users with atleast one other subreddit in common\n",
        "        other_users_id = get_ids_of_users_with_common_subreddits(user_id, users, used_subreddits)\n",
        "        for other_user_id in other_users_id:\n",
        "            # Save all UNIQUE common subreddits as edge property if constraints are satisfied\n",
        "            common_subreddits = get_common_subreddits(user_id, other_user_id, used_subreddits)\n",
        "            common_subreddits = list(set(common_subreddits))\n",
        "            # Remove 'from_subreddit' as common subreddit.\n",
        "            # TODO: Should we remove biden?\n",
        "            common_subreddits = list(filter(lambda e: e not in from_subreddits[user_id], common_subreddits))\n",
        "\n",
        "            if len(common_subreddits) >= n_required_subreddits:\n",
        "                G.add_edge(users[user_id], users[other_user_id], common_subreddits=(common_subreddits), \n",
        "                           weight=len(common_subreddits))\n",
        "    return G\n",
        "\n",
        "\n",
        "# Create graph\n",
        "G = create_graph(users, used_subreddits, from_subreddits, n_required_subreddits=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv1_ioY64Qwv"
      },
      "source": [
        "TODO: \n",
        "- Present basic NETWORK stats we got from Project A (#Edges, #nodes, avg/min/max degree)\n",
        "- Plot the network with (possibly with current classification = from_subreddit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-vpEwGrYnbA"
      },
      "source": [
        "## 3.2 Extracting networks of interest and classifying users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mgMRWJNZr-T"
      },
      "source": [
        "### 3.2.1 Extracting the \"backbone\" of the user network\n",
        "- Motive: \"As we saw from introduction - weighted is very dense... Might be able to extract to more informative!\" \n",
        "- Tools: \"Works by applying disperse filters, defined by \"...\n",
        "- Results: \"Resulting network is...\" (#Edges, #nodes, avg/min/max degree) + PLOT\n",
        "- Discussion: \" Will be used as comparison to weighted, to see which one gives more information\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g_LWaooF0hW"
      },
      "source": [
        "### 3.2.1.1\n",
        "\n",
        "We saw in section 3.1.1.1 that the weighted network was very dense. With such an overwhelming number of connections there will arguably be a lot of redundant information (Serrano 2009, 6483). \n",
        "\n",
        "Therefore it would be a good idea to attempt to reduce the complexity of the network while preserving the key features and information contained in the network. This can be seen as the network science analogy to performing principal component analysis on very high dimensional feature spaces known from machine learning. \n",
        "\n",
        "One immediate idea could be to preserve links having a certain weight and stripping links under this threshold from the network. However this kind of global thresholding would mean that we simply lose the information of the links with a weight under the somewhat arbitrary threshold value (Serrano 2009, 6483).\n",
        "\n",
        "A way to reduce the complexity in a more careful way is presented in the article Extracting the multiscale backbone of complex weighted networks (Serrano 2009). The core idea is to preserve the edges that are statistically significant for the network model in relation to a null hypothesis model where the weights are randomly uniform. A more detailed explanation will be provided in the following subsection. \n",
        "\n",
        "\n",
        "### 3.2.1.2\n",
        "We want to develop a so-called disparity filter to use as a tool for the complexity reduction. To develop this tool, the first thing needed is a way to let each node $i$ in the network assign a relative importance $p$ to its links:\n",
        "\n",
        "$$\n",
        "p_{ij} = \\frac{w_{ij}}{\\sum_j w_{ij}} \\quad \\text{for} \\quad i \\in I\n",
        "$$\n",
        "\n",
        "Thus for each link $j$ that a node has the formula above expresses the proportion of the nodes total weight that link $j$ carries. This allows us to express the level of local heterogenity or disparity in link weights (Serrano 2009, 6484). The next step is to consider what the disparity for a nodes weights should be expected to look like if the normalized weights $p_{ij}$ were placed uniformly random between 0 and 1. This means our null hypothesis is:\n",
        "\n",
        "$$\n",
        "\\mathbf{H_0}: \\text{For a node i with degree k,} \\quad p_{ij} \\sim U(0,1)\n",
        "$$\n",
        "\n",
        "Now we can take a certain node with degree $k$. Then we place $k-1$ points in the interval $[0,1]$ with uniform probability. \n",
        "\n",
        "The lengths of the resulting subintervals can be seen as the $p_{ij}$. They will represent the expected values of the $k$ normalized weights if they were randomly drawn from a uniform distribution (Serrano 2009, 6484).\n",
        "\n",
        "The probability distribution function for the $k-1$ poins taking a specific value $x$ is given by:\n",
        "\n",
        "$$\n",
        "p(x)dx = (k-1)(1-x)^{k-2}dx  \\quad \\text{(Serrano 2009, eq. 1)}\n",
        "$$\n",
        "\n",
        "It can be seen that this probability is dependent on the degree of the node in question. For example, for a node with two links, $k=2$ it can be seen that $p(x)dx=1dx$, which means that the single point dividing the interval into two lengths has uniform probability in the interval. \n",
        "\n",
        "For each node $i$ with links $j \\in J$ this means, if the null hypothesis, $\\mathbf{H_0}$ is true, that the probability of observing a normalized weight with a value as extreme or more extreme than $p_{ij}$ can be calculated by:\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} = 1 - (k-1)\\int_0^{p_{ij}}(1-x)^{k-2}dx \\quad \\text{(Serrano 2009, eq. 2)} \n",
        "$$\n",
        "\n",
        "A certain $p$ or $\\alpha$-value can then be set to form a criterion for when a link is significant. This means a link $j$ is significant if \n",
        "\n",
        "$$\n",
        "\\alpha_{ij} < \\alpha\n",
        "$$\n",
        "\n",
        "Again to provide a simple example with node degree $k=2$, it can be seen that the calculation of $\\alpha_{ij}$ simplifies to $\\alpha_{ij} = 1 - p_{ij}$. When $p_{ij}$ is close to 1, then link $j$ carries a lot of the total weight of the links connecting to node $i$ and so it should be an important link for node $i$. This is reflected by $\\alpha_{ij}$ which is small, meaning that it would be statistically significant to some level $\\alpha$.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jluN_r-JZ9fD"
      },
      "source": [
        "### 3.2.2 Classifying users with community Detection and sentiment analysis\n",
        "- Motive: \"Classifying users by from_subreddit is not necessarily optimal.\n",
        "- Tools: Three compared partitions: from_subreddit, Louvain and sentiment in comment. Modularity. Plots\n",
        "- Results: Modularity=... Plots=... #Links_Across_partition=..., MORE to decide the better partition!!?\n",
        "- Discussion: From X and Y we find __ as the best partitioning for representing each candidates' supporters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aYiNLPMZ9vr"
      },
      "source": [
        "### TBD: 3.2.3 Detecting communities with the bipartite network? ONLY MAYBE!\n",
        "- Motive: \"Bipartite networks might contain additional information, which is discarded in the projection\n",
        "- Tools: \"Explain how community detection works\"...\n",
        "- Results: \"We saw a lot more!!\" or \"revealed nothing...\"\n",
        "- Discussion: \"Probably because...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvM9MeznY0_U"
      },
      "source": [
        "## 3.3 Comparing candidate sub-networks (of best partitioning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P0QHXNHafM4"
      },
      "source": [
        "### 3.3.1 Simple Network Statistics for candidate sub-networks\n",
        "- Motive: \"To compare the two networks in terms of simple statistics\"\n",
        "- Tools: #Nodes, #Edges, Degrees, densities, median, mode, \n",
        "- Results: \"compute them...\"\n",
        "- Discussion: \"This could mean that... \"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNSc85poalnK"
      },
      "source": [
        "### 3.3.2 Degree Distributions and the Network types\n",
        "- Motive: \"To understand the characteristics of our networks... Does our network follow power-law? Which could mean...\"\n",
        "- Tools: explain theory...\n",
        "- Results\n",
        "- Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc5vt1ZsasVZ"
      },
      "source": [
        "### 3.3.3 Advanced statistics (maybe this should be 3 seperate bullets)\n",
        "- Motive: \"Which supporters are more diverse in interests? which are etc...\n",
        "- Tools: Clustering, Shortest paths and centralities in sub-networks?\n",
        "- Results\n",
        "- Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP9TizGuay43"
      },
      "source": [
        "### 3.3.4 Community detection wihin partitions\n",
        "- Motive: \"Investigate if any communities within Biden/trump lair\"\n",
        "- Tools: Louvain\n",
        "- Results\n",
        "- Dicussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMXEh92ha3_F"
      },
      "source": [
        "## 3.4 Comparing text/comments of candidates' supporters/subreddit forum users "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8SyyqKpbnap"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWTCXdaLbUN8"
      },
      "source": [
        "### 3.4.1 Natural Language Processing\n",
        "- Motive: Is one community more eloquent? Does either community have more catch-phrases? Typical words?\n",
        "- Tools: Lexical diversity, collocations, TFTR + wordclouds\n",
        "- Results\n",
        "- Discussion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vi3lYFYbfvk"
      },
      "source": [
        "### 3.4.2 Sentiment analysis\n",
        "- Motive: Is one candidate´s supporters more positive than the other's?\n",
        "- Tools: Sentiment analysis of comments\n",
        "- Results\n",
        "- Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck5dlYnLVNkv"
      },
      "source": [
        "# 4 Discussion. Think critically about your creation\n",
        "- What went well?,\n",
        "- What is still missing? What could be improved?, Why?\n",
        "  - Improvements: Maybe also try to incooporate how *often* the users have commented on the same subreddit in link weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khF8pi-_Zbeu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf4nX63WVdeX"
      },
      "source": [
        "# 5 Contributions. Who did what?\n",
        "- You should write (just briefly) which group member was the main responsible for which elements of the assignment. (I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work. That’s what you should explain).\n",
        "\n",
        "- It is not OK simply to write \"All group members contributed equally\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko2BgVOeVYHZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}